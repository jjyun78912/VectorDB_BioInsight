#!/usr/bin/env python3
"""
VectorDB ì¸ë±ì‹± ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

ë…¼ë¬¸ë³„ ì„¹ì…˜ ë¶„ë¦¬, ì²­í‚¹, ë©”íƒ€ë°ì´í„°ê°€ ì œëŒ€ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
"""
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.vector_store import create_vector_store
from collections import defaultdict
import json


def verify_indexing():
    """ì¸ë±ì‹± ìƒíƒœë¥¼ ìƒì„¸ ê²€ì¦"""

    print("=" * 70)
    print("VectorDB ì¸ë±ì‹± ê²€ì¦ ë¦¬í¬íŠ¸")
    print("=" * 70)

    # Vector Store ì—°ê²°
    vs = create_vector_store(disease_domain="pheochromocytoma")

    # ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    all_data = vs.collection.get(
        include=["documents", "metadatas", "embeddings"]
    )

    total_chunks = len(all_data["ids"])
    print(f"\nì´ ì €ì¥ëœ Chunks: {total_chunks}")
    embeddings = all_data.get('embeddings')
    emb_dim = len(embeddings[0]) if embeddings is not None and len(embeddings) > 0 else 'N/A'
    print(f"Embedding ì°¨ì›: {emb_dim}")

    # =========================================
    # 1. ë…¼ë¬¸ë³„ ë¶„ì„
    # =========================================
    print("\n" + "=" * 70)
    print("1. ë…¼ë¬¸ë³„ Chunk ë¶„í¬")
    print("=" * 70)

    papers = defaultdict(lambda: {"chunks": 0, "sections": defaultdict(int)})

    for i, meta in enumerate(all_data["metadatas"]):
        paper_title = meta.get("paper_title", "Unknown")[:50]
        section = meta.get("section", "Unknown")
        papers[paper_title]["chunks"] += 1
        papers[paper_title]["sections"][section] += 1
        papers[paper_title]["doi"] = meta.get("doi", "")

    for paper, info in papers.items():
        print(f"\nğŸ“„ {paper}...")
        print(f"   DOI: {info['doi']}")
        print(f"   ì´ Chunks: {info['chunks']}")
        print(f"   ì„¹ì…˜ë³„ ë¶„í¬:")
        for section, count in sorted(info["sections"].items()):
            print(f"      - {section}: {count} chunks")

    # =========================================
    # 2. ì„¹ì…˜ë³„ ë¶„ì„
    # =========================================
    print("\n" + "=" * 70)
    print("2. ì „ì²´ ì„¹ì…˜ë³„ Chunk ë¶„í¬")
    print("=" * 70)

    section_counts = defaultdict(int)
    for meta in all_data["metadatas"]:
        section = meta.get("section", "Unknown")
        section_counts[section] += 1

    for section, count in sorted(section_counts.items(), key=lambda x: -x[1]):
        pct = (count / total_chunks) * 100
        bar = "â–ˆ" * int(pct / 2)
        print(f"  {section:25} {count:4} ({pct:5.1f}%) {bar}")

    # =========================================
    # 3. ë©”íƒ€ë°ì´í„° í•„ë“œ í™•ì¸
    # =========================================
    print("\n" + "=" * 70)
    print("3. ë©”íƒ€ë°ì´í„° í•„ë“œ ê²€ì¦")
    print("=" * 70)

    required_fields = ["paper_title", "section", "doi", "disease_domain", "chunk_index", "source_file"]
    field_coverage = {field: 0 for field in required_fields}

    for meta in all_data["metadatas"]:
        for field in required_fields:
            if meta.get(field):
                field_coverage[field] += 1

    print("\n  í•„ë“œëª…                    | ì±„ì›Œì§„ ë¹„ìœ¨")
    print("  " + "-" * 45)
    for field, count in field_coverage.items():
        pct = (count / total_chunks) * 100
        status = "âœ…" if pct > 90 else "âš ï¸" if pct > 50 else "âŒ"
        print(f"  {status} {field:22} | {count:4}/{total_chunks} ({pct:.1f}%)")

    # =========================================
    # 4. ìƒ˜í”Œ Chunk í™•ì¸
    # =========================================
    print("\n" + "=" * 70)
    print("4. ìƒ˜í”Œ Chunk ìƒì„¸ í™•ì¸ (ê° ì„¹ì…˜ë³„ 1ê°œ)")
    print("=" * 70)

    shown_sections = set()
    for i, (doc, meta) in enumerate(zip(all_data["documents"], all_data["metadatas"])):
        section = meta.get("section", "Unknown")
        if section not in shown_sections and len(shown_sections) < 5:
            shown_sections.add(section)
            print(f"\n--- [{section}] ì„¹ì…˜ ìƒ˜í”Œ ---")
            print(f"Paper: {meta.get('paper_title', 'Unknown')[:60]}...")
            print(f"Chunk Index: {meta.get('chunk_index', 'N/A')}")
            print(f"Disease Domain: {meta.get('disease_domain', 'N/A')}")
            print(f"Content Preview ({len(doc)} chars):")
            print(f"  \"{doc[:200]}...\"")

    # =========================================
    # 5. Embedding ê²€ì¦
    # =========================================
    print("\n" + "=" * 70)
    print("5. Embedding ë²¡í„° ê²€ì¦")
    print("=" * 70)

    if embeddings is not None and len(embeddings) > 0:
        sample_emb = embeddings[0]
        print(f"\n  Embedding ì°¨ì›: {len(sample_emb)}")
        print(f"  ìƒ˜í”Œ ë²¡í„° (ì²˜ìŒ 10ê°œ ê°’):")
        print(f"  {sample_emb[:10]}")

        # ë²¡í„° ì •ê·œí™” í™•ì¸
        import math
        magnitude = math.sqrt(sum(x**2 for x in sample_emb))
        print(f"  ë²¡í„° í¬ê¸° (L2 norm): {magnitude:.4f}")

    # =========================================
    # 6. ìš”ì•½
    # =========================================
    print("\n" + "=" * 70)
    print("6. ê²€ì¦ ìš”ì•½")
    print("=" * 70)

    issues = []
    if len(section_counts) < 3:
        issues.append("âš ï¸ ì„¹ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡± (3ê°œ ë¯¸ë§Œ)")
    if field_coverage["section"] / total_chunks < 0.9:
        issues.append("âš ï¸ ì„¹ì…˜ ë©”íƒ€ë°ì´í„° ëˆ„ë½ ìˆìŒ")
    if embeddings is None or len(embeddings) == 0:
        issues.append("âŒ Embeddingì´ ì €ì¥ë˜ì§€ ì•ŠìŒ")

    if not issues:
        print("\n  âœ… ëª¨ë“  ê²€ì¦ í†µê³¼!")
        print(f"  - {len(papers)}ê°œ ë…¼ë¬¸ ì¸ë±ì‹± ì™„ë£Œ")
        print(f"  - {len(section_counts)}ê°œ ì„¹ì…˜ ìœ í˜• ê°ì§€")
        print(f"  - {total_chunks}ê°œ Chunk + Embedding ì €ì¥")
        print(f"  - ë©”íƒ€ë°ì´í„° ì •ìƒ ì €ì¥")
    else:
        print("\n  ë°œê²¬ëœ ì´ìŠˆ:")
        for issue in issues:
            print(f"    {issue}")

    print("\n" + "=" * 70)


if __name__ == "__main__":
    verify_indexing()
